{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd #For working with data tables and dataframes\n",
        "import numpy as np #Fro numerical operations\n",
        "import matplotlib.pyplot as plt #For making plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1754410850480
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'infer_column_types': 'False', 'activity': 'to_pandas_dataframe'}\n",
            "{'infer_column_types': 'False', 'activity': 'to_pandas_dataframe', 'activityApp': 'TabularDataset'}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>observation_date</th>\n",
              "      <th>DGS10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1962-01-02</td>\n",
              "      <td>4.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1962-01-03</td>\n",
              "      <td>4.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1962-01-04</td>\n",
              "      <td>3.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1962-01-05</td>\n",
              "      <td>4.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1962-01-08</td>\n",
              "      <td>4.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16575</th>\n",
              "      <td>2025-07-15</td>\n",
              "      <td>4.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16576</th>\n",
              "      <td>2025-07-16</td>\n",
              "      <td>4.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16577</th>\n",
              "      <td>2025-07-17</td>\n",
              "      <td>4.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16578</th>\n",
              "      <td>2025-07-18</td>\n",
              "      <td>4.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16579</th>\n",
              "      <td>2025-07-21</td>\n",
              "      <td>4.38</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16580 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      observation_date  DGS10\n",
              "0           1962-01-02   4.06\n",
              "1           1962-01-03   4.03\n",
              "2           1962-01-04   3.99\n",
              "3           1962-01-05   4.02\n",
              "4           1962-01-08   4.03\n",
              "...                ...    ...\n",
              "16575       2025-07-15   4.50\n",
              "16576       2025-07-16   4.46\n",
              "16577       2025-07-17   4.47\n",
              "16578       2025-07-18   4.44\n",
              "16579       2025-07-21   4.38\n",
              "\n",
              "[16580 rows x 2 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from azureml.core import Workspace, Dataset\n",
        "\n",
        "subscription_id = 'b23ddd02-1d4d-4c80-8ef3-f68a97a0dab6'\n",
        "resource_group = 'MLDev'\n",
        "workspace_name = 'project_groundhog'\n",
        "\n",
        "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
        "\n",
        "dataset = Dataset.get_by_name(workspace, name='Treasury')\n",
        "dataset.to_pandas_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The power of the LSTM is in its cell. The cell, in an RNN, is basically the fundamental building block that processes sequential data. It takes in both the current input and the previous cell's output (the hidden state) as input,  and produces a new output and hidden state. \n",
        "\n",
        "The gates in an LSTM are what allows it to maintain a long and short term memory. There are 3. \n",
        "\n",
        "The Input Gate decides how much new information to keep in the cell state. It uses a sigmoid function tofilter incoming data, determining what weights and biases need to be updated.\n",
        "\n",
        "The Forget Gate determines how much old information to remove from the cell state. It also uses a sigmoid function to review current inputs and past outputs, and decides wether to retain or discard previous states. The goal is to keep the network free form unnecessary/irrelivant data/\n",
        "\n",
        "The Output Gate manages what the next hidden layer or the output will receive from the cell state. It uses tanh function on the filtered cell state to scale the values, deciding, based on the sigmoid's state what should be passed onto the output.\n",
        "\n",
        "This will be shown later"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "WeightInitializer\n",
        "- Blueprint for initializing weights for the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WeightInitializer: \n",
        "\n",
        "    #setup function that runs when yyou create a new weight initializer. It remembers the method you want to use for initializing weights, an ddefaults to 'random' if you don't specify one.\n",
        "\n",
        "    def __init__(self, method='random'): \n",
        "        self.method = method\n",
        "\n",
        "    #This function creates the actual weights. The \"shape\" tells it how many weights to create and how to arrange them\n",
        "\n",
        "    def initialize(self, shape):\n",
        "        if self.method == 'random':\n",
        "            return np.random.randn(*shape) #Creates random numbers from a bell curve\n",
        "        elif self.method == 'xavier':\n",
        "            return np.random.randn(*shape) / np.sqrt(shape[0]) #Divides those random numbers by the square root of the first dimension - this helps prevent numbers from getting too big or small\n",
        "        elif self.method == 'he':\n",
        "            return np.random.randn(*shape) * np.sqrt(2. / shape[0]) #Multiplies random numbers by a specific factor\n",
        "        elif self.method == 'uniform':\n",
        "            return np.random.uniform(-1, 1, shape) #Creates random numbers evenly spread between -1 and 1\n",
        "        else:\n",
        "            raise ValueError(\"Unknown initialization method: {}\".format(self.method))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PlotManager\n",
        "- Blueprint for plotting loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PlotManager:\n",
        "\n",
        "    #This sets up a figure with space for 3 graphs stacked vertically, sized 6 inches wide by 4 inches tall.\n",
        "\n",
        "    def __init__(self):\n",
        "        self.fig, self.ax = plt.subplots(3, 1, figsize=(6, 4))\n",
        "\n",
        "    #This function draws two lines on a graph: One line showing how training loss changed over time. Another line showing validation loss (how well the model performs on data it hasn't seen)\n",
        "\n",
        "    def plot_losses(self, train_losses, val_losses):\n",
        "        self.ax.plot(train_losses, label='Training Loss')\n",
        "        self.ax.plot(val_losses, label='Validation Loss')\n",
        "        self.ax.set_title('Training and Validation Losses')\n",
        "        self.ax.set_xlabel('Epoch')\n",
        "        self.ax.set_ylabel('Loss')\n",
        "        self.ax.legend()\n",
        "\n",
        "    #shows the plots\n",
        "\n",
        "    def show_plots(self):\n",
        "        plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EarlyStopping\n",
        "- Blueprint for early stopping after a given epoch, in order to prevent overfitting. \n",
        "- This class monitors the validation loss and stops training if it does not improve for a specified number of epochs (patience).\n",
        "- It can also print messages when the loss does not improve, depending on the verbose flag.\n",
        "Args:\n",
        "- patience (int): Number of epochs to wait before stopping the training. Default is 7.\n",
        "- verbose (bool): If True, prints a message for each epoch where the loss does not improve.\n",
        "- delta (float): Minimum change in the monitored quantity to qualify as an improvement. Default is 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        self.patience = patience #Number of epochs to wait before stopping\n",
        "        self.verbose = verbose #If True, prints messages when the loss does not improve\n",
        "        self.counter = 0 #Counter for how many epochs the loss has not improved\n",
        "        self.best_score = None #Best score seen so far\n",
        "        self.early_stop = False #Flag to indicate if training should stop\n",
        "        self.delta = delta #Minimum change in the monitored quantity to qualify as an improvement\n",
        "\n",
        "    #This function is called at the end of each epoch to check if the model should stop training based on the validation loss.\n",
        "    #If the validation loss does not improve for a number of epochs equal to patience, it sets early_stop to True.\n",
        "    #If verbose is True, it prints a message indicating that the loss did not improve.\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        #Determines if the model should stop training.\n",
        "\n",
        "        # Args: val_loss (float): The loss of the model on the validation set.\n",
        "        \n",
        "        score = -val_loss #flips the sign of the validation loss to make it a score (lower loss is better)\n",
        "\n",
        "        if self.best_score is None: \n",
        "            self.best_score = score # self.best_score is set to the first score seen\n",
        "\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1 # If the score has not improved by at least delta, increment the counter\n",
        "\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True # If the counter exceeds patience, set early_stop to True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.counter = 0 # If the score has improved, reset the counter to 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Class for LSTM Network\n",
        "\n",
        "    - input_size: int, dimensionality of input space\n",
        "    - hidden_size: int, number of LSTM units\n",
        "    - output_size: int, dimensionality of output space\n",
        "    - init_method: str, weight initialization method (default: 'xavier')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LSTM:\n",
        "    \"\"\"\n",
        "    Long Short-Term Memory (LSTM) network.\n",
        "\n",
        "    Parameters:\n",
        "    - input_size: int, dimensionality of input space\n",
        "    - hidden_size: int, number of LSTM units\n",
        "    - output_size: int, dimensionality of output space\n",
        "    - init_method: str, weight initialization method (default: 'xavier')\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, output_size, init_method='xavier'):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.weight_initializer = WeightInitializer(method=init_method)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.wf = self.weight_initializer.initialize((hidden_size, hidden_size + input_size)) # Forget gate weights\n",
        "        self.wi = self.weight_initializer.initialize((hidden_size, hidden_size + input_size)) # Input gate weights\n",
        "        self.wo = self.weight_initializer.initialize((hidden_size, hidden_size + input_size)) # Output gate weights\n",
        "        self.wc = self.weight_initializer.initialize((hidden_size, hidden_size + input_size)) # Candiate gate weights. Creates new candidate information to potentially store in the cell state\n",
        "\n",
        "        # Initialize biases for all gates starting at zero--industry standard\n",
        "        self.bf = np.zeros((hidden_size, 1)) \n",
        "        self.bi = np.zeros((hidden_size, 1))\n",
        "        self.bo = np.zeros((hidden_size, 1))\n",
        "        self.bc = np.zeros((hidden_size, 1))\n",
        "\n",
        "        # Initialize output layer weights and biases. \n",
        "        # These are for the final output of the LSTM network, which maps the hidden state to the desired output space.\n",
        "        self.why = self.weight_initializer.initialize((output_size, hidden_size)) \n",
        "        self.by = np.zeros((output_size, 1))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(z):\n",
        "        \"\"\"\n",
        "        Sigmoid activation function.\n",
        "\n",
        "        Parameters:\n",
        "        - z: np.ndarray, input to the activation function\n",
        "\n",
        "        Returns:\n",
        "        - np.ndarray, output of the activation function\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    @staticmethod\n",
        "    def dsigmoid(y):\n",
        "        \"\"\"\n",
        "        Derivative of the sigmoid activation function.\n",
        "\n",
        "        Parameters:\n",
        "        - y: np.ndarray, output of the sigmoid activation function\n",
        "\n",
        "        Returns:\n",
        "        - np.ndarray, derivative of the sigmoid function\n",
        "        \"\"\"\n",
        "        return y * (1 - y)\n",
        "\n",
        "    @staticmethod\n",
        "    def dtanh(y):\n",
        "        \"\"\"\n",
        "        Derivative of the hyperbolic tangent activation function.\n",
        "\n",
        "        Parameters:\n",
        "        - y: np.ndarray, output of the hyperbolic tangent activation function\n",
        "\n",
        "        Returns:\n",
        "        - np.ndarray, derivative of the hyperbolic tangent function\n",
        "        \"\"\"\n",
        "        return 1 - y * y\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # This function processes input data through the LSTM network, moving one time step at a time.\n",
        "\n",
        "        \"\"\"\n",
        "        Forward pass through the LSTM network.\n",
        "\n",
        "        Parameters:\n",
        "        - x: np.ndarray, input to the network\n",
        "\n",
        "        Returns:\n",
        "        - np.ndarray, output of the network\n",
        "        - list, caches containing intermediate values for backpropagation\n",
        "        \"\"\"\n",
        "        caches = [] # List to store intermediate values for backpropagation\n",
        "        h_prev = np.zeros((self.hidden_size, 1)) #previous hidden state. Short term memory of the LSTM\n",
        "        c_prev = np.zeros((self.hidden_size, 1)) # Previous cell state. Long term memory of the LSTM\n",
        "        h = h_prev \n",
        "        c = c_prev\n",
        "\n",
        "        # Loop through each time step in the input sequence\n",
        "        # x.shape[0] is the number of time steps in the input sequence\n",
        "        # x[t] is the input at time step t, reshaped to match the expected input shape\n",
        "        # combined is the concatenation of the previous hidden state and the current input at time step t\n",
        "        # f, i, o, c_ are the forget, input, output gates and candidate cell state respectively\n",
        "        # c is the new cell state and h is the new hidden state\n",
        "        # The caches list stores all intermediate values needed for backpropagation\n",
        "        # The final output y is computed by multiplying the hidden state h with the output layer weights and adding the output layer bias\n",
        "        # The function returns the output y and the caches list\n",
        "\n",
        "\n",
        "        #Add in for loop to forget weights after 60 steps\n",
        "\n",
        "        for t in range(x.shape[0]): \n",
        "            x_t = x[t].reshape(-1, 1)\n",
        "            combined = np.vstack((h_prev, x_t))\n",
        "\n",
        "            f = self.sigmoid(np.dot(self.wf, combined) + self.bf) # Decides what to forget from long term memory. \n",
        "            i = self.sigmoid(np.dot(self.wi, combined) + self.bi) # How much of the new information to store\n",
        "            o = self.sigmoid(np.dot(self.wo, combined) + self.bo) # What parts of the memory to output\n",
        "            c_ = np.tanh(np.dot(self.wc, combined) + self.bc) # deals with new candidate information to potentially store in the cell state\n",
        "\n",
        "            c = f * c_prev + i * c_ #New long-term memory = (forget gate × old memory) + (input gate × new candidate)\n",
        "            h = o * np.tanh(c) #New short-term memory = output gate × processed long-term memory\n",
        "\n",
        "            cache = (h_prev, c_prev, f, i, o, c_, x_t, combined, c, h) #Saves all intermediate values needed for backpropagation\n",
        "            caches.append(cache)\n",
        "\n",
        "            h_prev, c_prev = h, c # Update previous hidden and cell states for the next time step\n",
        "\n",
        "        y = np.dot(self.why, h) + self.by # Final output of the network, computed by multiplying the hidden state with the output layer weights and adding the output layer bias\n",
        "        return y, caches\n",
        "\n",
        "    def backward(self, dy, caches, clip_value=1.0):\n",
        "        \"\"\"\n",
        "        Backward pass through the LSTM network.\n",
        "\n",
        "        Parameters:\n",
        "        - dy: np.ndarray, gradient of the loss with respect to the output\n",
        "        - caches: list, caches from the forward pass\n",
        "        - clip_value: float, value to clip gradients to (default: 1.0)\n",
        "\n",
        "        Returns:\n",
        "        - tuple, gradients of the loss with respect to the parameters\n",
        "        \"\"\"\n",
        "        dWf, dWi, dWo, dWc = [np.zeros_like(w) for w in (self.wf, self.wi, self.wo, self.wc)] #create empty arrays for gradients of weights. \n",
        "            # np.zeros_like(w) means \"make an array of zeros that's the exact same size and shape as self.wf\"\n",
        "        dbf, dbi, dbo, dbc = [np.zeros_like(b) for b in (self.bf, self.bi, self.bo, self.bc)] #create empty arrays for gradients of biases\n",
        "        dWhy = np.zeros_like(self.why)\n",
        "        dby = np.zeros_like(self.by)\n",
        "\n",
        "        # Ensure dy is reshaped to match output size\n",
        "        dy = dy.reshape(self.output_size, -1)\n",
        "        dh_next = np.zeros((self.hidden_size, 1))  # shape must match hidden_size\n",
        "        dc_next = np.zeros_like(dh_next) # shape must match hidden_size\n",
        "\n",
        "        for cache in reversed(caches):\n",
        "            h_prev, c_prev, f, i, o, c_, x_t, combined, c, h = cache # For each time step, we are unpacking the cached values from the forward pass\n",
        "\n",
        "            # Add gradient from next step to current output gradient\n",
        "            dh = np.dot(self.why.T, dy) + dh_next # Calculates how much the hidden state should change based on the output gradient and the previous hidden state gradient\n",
        "            dc = dc_next + (dh * o * self.dtanh(np.tanh(c))) # How much the cell state should change based on the hidden state gradient and the output gate\n",
        "        \n",
        "            # Calculate gradients for each gate. Figuring out how much of the blame for the error should go to each gate\n",
        "            # df, di, do, dc_ are the gradients for the forget gate, input gate, output gate, and candidate cell state respectively\n",
        "            # They are calculated using the chain rule and the derivatives of the activation functions\n",
        "            df = dc * c_prev * self.dsigmoid(f)\n",
        "            di = dc * c_ * self.dsigmoid(i)\n",
        "            do = dh * self.dtanh(np.tanh(c))\n",
        "            dc_ = dc * i * self.dtanh(c_)\n",
        "\n",
        "            #\n",
        "\n",
        "            dcombined_f = np.dot(self.wf.T, df)\n",
        "            dcombined_i = np.dot(self.wi.T, di)\n",
        "            dcombined_o = np.dot(self.wo.T, do)\n",
        "            dcombined_c = np.dot(self.wc.T, dc_)\n",
        "\n",
        "            dcombined = dcombined_f + dcombined_i + dcombined_o + dcombined_c\n",
        "            dh_next = dcombined[:self.hidden_size]\n",
        "            dc_next = f * dc\n",
        "\n",
        "            dWf += np.dot(df, combined.T)\n",
        "            dWi += np.dot(di, combined.T)\n",
        "            dWo += np.dot(do, combined.T)\n",
        "            dWc += np.dot(dc_, combined.T)\n",
        "\n",
        "            dbf += df.sum(axis=1, keepdims=True)\n",
        "            dbi += di.sum(axis=1, keepdims=True)\n",
        "            dbo += do.sum(axis=1, keepdims=True)\n",
        "            dbc += dc_.sum(axis=1, keepdims=True)\n",
        "\n",
        "        dWhy += np.dot(dy, h.T)\n",
        "        dby += dy\n",
        "\n",
        "        gradients = (dWf, dWi, dWo, dWc, dbf, dbi, dbo, dbc, dWhy, dby)\n",
        "\n",
        "        # Gradient clipping\n",
        "        for i in range(len(gradients)):\n",
        "            np.clip(gradients[i], -clip_value, clip_value, out=gradients[i])\n",
        "\n",
        "        return gradients\n",
        "\n",
        "    def update_params(self, grads, learning_rate):\n",
        "        \"\"\"\n",
        "        Update the parameters of the network using the gradients.\n",
        "\n",
        "        Parameters:\n",
        "        - grads: tuple, gradients of the loss with respect to the parameters\n",
        "        - learning_rate: float, learning rate\n",
        "        \"\"\"\n",
        "        dWf, dWi, dWo, dWc, dbf, dbi, dbo, dbc, dWhy, dby = grads\n",
        "\n",
        "        self.wf -= learning_rate * dWf\n",
        "        self.wi -= learning_rate * dWi\n",
        "        self.wo -= learning_rate * dWo\n",
        "        self.wc -= learning_rate * dWc\n",
        "\n",
        "        self.bf -= learning_rate * dbf\n",
        "        self.bi -= learning_rate * dbi\n",
        "        self.bo -= learning_rate * dbo\n",
        "        self.bc -= learning_rate * dbc\n",
        "\n",
        "        self.why -= learning_rate * dWhy\n",
        "        self.by -= learning_rate * dby"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LSTMTrainer:\n",
        "    \"\"\"\n",
        "    Trainer for the LSTM network.\n",
        "\n",
        "    Parameters:\n",
        "    - model: LSTM, the LSTM network to train\n",
        "    - learning_rate: float, learning rate for the optimizer\n",
        "    - patience: int, number of epochs to wait before early stopping\n",
        "    - verbose: bool, whether to print training information\n",
        "    - delta: float, minimum change in validation loss to qualify as an improvement\n",
        "    \"\"\"\n",
        "    def __init__(self, model, learning_rate=0.01, patience=7, verbose=True, delta=0):\n",
        "        self.model = model\n",
        "        self.learning_rate = learning_rate\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.early_stopping = EarlyStopping(patience, verbose, delta)\n",
        "\n",
        "    def train(self, X_train, y_train, X_val=None, y_val=None, epochs=10, batch_size=1, clip_value=1.0):\n",
        "        \"\"\"\n",
        "        Train the LSTM network.\n",
        "\n",
        "        Parameters:\n",
        "        - X_train: np.ndarray, training data\n",
        "        - y_train: np.ndarray, training labels\n",
        "        - X_val: np.ndarray, validation data\n",
        "        - y_val: np.ndarray, validation labels\n",
        "        - epochs: int, number of training epochs\n",
        "        - batch_size: int, size of mini-batches\n",
        "        - clip_value: float, value to clip gradients to\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            epoch_losses = []\n",
        "            for i in range(0, len(X_train), batch_size):\n",
        "                batch_X = X_train[i:i + batch_size]\n",
        "                batch_y = y_train[i:i + batch_size]\n",
        "                losses = []\n",
        "\n",
        "                for x, y_true in zip(batch_X, batch_y):\n",
        "                    y_pred, caches = self.model.forward(x)\n",
        "                    loss = self.compute_loss(y_pred, y_true.reshape(-1, 1))\n",
        "                    losses.append(loss)\n",
        "\n",
        "                    # Backpropagation to get gradients\n",
        "                    dy = y_pred - y_true.reshape(-1, 1)\n",
        "                    grads = self.model.backward(dy, caches, clip_value=clip_value)\n",
        "                    self.model.update_params(grads, self.learning_rate)\n",
        "\n",
        "                batch_loss = np.mean(losses)\n",
        "                epoch_losses.append(batch_loss)\n",
        "\n",
        "            avg_epoch_loss = np.mean(epoch_losses)\n",
        "            self.train_losses.append(avg_epoch_loss)\n",
        "\n",
        "            if X_val is not None and y_val is not None:\n",
        "                val_loss = self.validate(X_val, y_val)\n",
        "                self.val_losses.append(val_loss)\n",
        "                print(f'Epoch {epoch + 1}/{epochs} - Loss: {avg_epoch_loss:.5f}, Val Loss: {val_loss:.5f}')\n",
        "\n",
        "                # Check early stopping condition\n",
        "                self.early_stopping(val_loss)\n",
        "                if self.early_stopping.early_stop:\n",
        "                    print(\"Early stopping\")\n",
        "                    break\n",
        "            else:\n",
        "                print(f'Epoch {epoch + 1}/{epochs} - Loss: {avg_epoch_loss:.5f}')\n",
        "\n",
        "    def compute_loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Compute mean squared error loss.\n",
        "        \"\"\"\n",
        "        return np.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "    def validate(self, X_val, y_val):\n",
        "        \"\"\"\n",
        "        Validate the model on a separate set of data.\n",
        "        \"\"\"\n",
        "        val_losses = []\n",
        "        for x, y_true in zip(X_val, y_val):\n",
        "            y_pred, _ = self.model.forward(x)\n",
        "            loss = self.compute_loss(y_pred, y_true.reshape(-1, 1))\n",
        "            val_losses.append(loss)\n",
        "        return np.mean(val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (2334281612.py, line 27)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 27\u001b[0;36m\u001b[0m\n\u001b[0;31m    df = df[(df['Date'] >= self.start_date) &amp; (df['Date'] <= self.end_date)]\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "class TimeSeriesDataset:\n",
        "    \"\"\"\n",
        "    Dataset class for time series data.\n",
        "\n",
        "    Parameters:\n",
        "    - ticker: str, stock ticker symbol\n",
        "    - start_date: str, start date for data retrieval\n",
        "    - end_date: str, end date for data retrieval\n",
        "    - look_back: int, number of previous time steps to include in each sample\n",
        "    - train_size: float, proportion of data to use for training\n",
        "    \"\"\"\n",
        "    def __init__(self, start_date, end_date, look_back=1, train_size=0.67):\n",
        "        self.start_date = start_date\n",
        "        self.end_date = end_date\n",
        "        self.look_back = look_back\n",
        "        self.train_size = train_size\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"\n",
        "        Load stock data.\n",
        "\n",
        "        Returns:\n",
        "        - np.ndarray, training data\n",
        "        - np.ndarray, testing data\n",
        "        \"\"\"\n",
        "        df = pd.read_csv('data/google.csv')\n",
        "        df = df[(df['Date'] >= self.start_date) &amp; (df['Date'] <= self.end_date)]\n",
        "        df = df.sort_index()\n",
        "        df = df.loc[self.start_date:self.end_date]\n",
        "        df = df[['Close']].astype(float)  # Use closing price\n",
        "        df = self.MinMaxScaler(df.values)  # Convert DataFrame to numpy array\n",
        "        train_size = int(len(df) * self.train_size)\n",
        "        train, test = df[0:train_size,:], df[train_size:len(df),:]\n",
        "        return train, test\n",
        "\n",
        "    def MinMaxScaler(self, data):\n",
        "        \"\"\"\n",
        "        Min-max scaling of the data.\n",
        "\n",
        "        Parameters:\n",
        "        - data: np.ndarray, input data\n",
        "        \"\"\"\n",
        "        numerator = data - np.min(data, 0)\n",
        "        denominator = np.max(data, 0) - np.min(data, 0)\n",
        "        return numerator / (denominator + 1e-7)\n",
        "\n",
        "    def create_dataset(self, dataset):\n",
        "        \"\"\"\n",
        "        Create the dataset for time series prediction.\n",
        "\n",
        "        Parameters:\n",
        "        - dataset: np.ndarray, input data\n",
        "\n",
        "        Returns:\n",
        "        - np.ndarray, input data\n",
        "        - np.ndarray, output data\n",
        "        \"\"\"\n",
        "        dataX, dataY = [], []\n",
        "        for i in range(len(dataset)-self.look_back):\n",
        "            a = dataset[i:(i + self.look_back), 0]\n",
        "            dataX.append(a)\n",
        "            dataY.append(dataset[i + self.look_back, 0])\n",
        "        return np.array(dataX), np.array(dataY)\n",
        "\n",
        "    def get_train_test(self):\n",
        "        \"\"\"\n",
        "        Get the training and testing data.\n",
        "\n",
        "        Returns:\n",
        "        - np.ndarray, training input\n",
        "        - np.ndarray, training output\n",
        "        - np.ndarray, testing input\n",
        "        - np.ndarray, testing output\n",
        "        \"\"\"\n",
        "        train, test = self.load_data()\n",
        "        trainX, trainY = self.create_dataset(train)\n",
        "        testX, testY = self.create_dataset(test)\n",
        "        return trainX, trainY, testX, testY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate the dataset\n",
        "dataset = TimeSeriesDataset( '2010-1-1', '2020-12-31', look_back=1)\n",
        "trainX, trainY, testX, testY = dataset.get_train_test()\n",
        "\n",
        "# Reshape input to be [samples, time steps, features]\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
        "testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
        "\n",
        "look_back = 1  # Number of previous time steps to include in each sample\n",
        "hidden_size = 256  # Number of LSTM units\n",
        "output_size = 1  # Dimensionality of the output space\n",
        "\n",
        "lstm = LSTM(input_size=1, hidden_size=hidden_size, output_size=output_size)\n",
        "\n",
        "trainer = LSTMTrainer(lstm, learning_rate=1e-3, patience=50, verbose=True, delta=0.001)\n",
        "trainer.train(trainX, trainY, testX, testY, epochs=1000, batch_size=32)\n",
        "\n",
        "plot_manager = PlotManager()\n",
        "\n",
        "# Inside your training loop\n",
        "plot_manager.plot_losses(trainer.train_losses, trainer.val_losses)\n",
        "\n",
        "# After your training loop\n",
        "plot_manager.show_plots()"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
