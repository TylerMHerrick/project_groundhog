{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e11b3f0",
   "metadata": {},
   "source": [
    "Import the excel file into a workable dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad6be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('FRED.xlsx')\n",
    "df.dropna(subset=['DGS10'], inplace=True)  # Remove rows where DGS10 is null\n",
    "df.rename(columns={'observation_date': 'Date'}, inplace=True)\n",
    "display(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac774a0a",
   "metadata": {},
   "source": [
    "Plot the data to see what we are dealing with--visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6b6391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df['Date'], df['DGS10'])\n",
    "plt.title('FRED Data Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('DGS10')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2954a6ed",
   "metadata": {},
   "source": [
    "Model for one step ahead prediction, which uses a sliding window and trains off of test data periodically.\n",
    "- this is not exactly what we want for long term future predictions. \n",
    "- This is to predict one value ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6296900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "def predict_and_evaluate_column_one_step_ahead(column_name):\n",
    "\n",
    "    window = 60 #This sets the number of months the model looks at at a time\n",
    "    training_data_percent = .8 #This is the % of data that we show to the model to train\n",
    "\n",
    "    industry = df.filter([column_name]) #Select the industry/niche we want to look at\n",
    "    dataset = industry.values #^\n",
    "    training_data_len = int(np.ceil( len(dataset) * training_data_percent )) #Show the model the data\n",
    "\n",
    "    scaler = StandardScaler() #transform the data into a format the model can work with\n",
    "    scaled_data = scaler.fit_transform(dataset) #normalize that data (Models work better with data -1:1)\n",
    "\n",
    "    training_data = scaled_data[:training_data_len] #Take only the scaled data into account\n",
    "\n",
    "    x_train, y_train = [], [] #create bins for the model to fill\n",
    "    for i in range(window, len(training_data)):\n",
    "        x_train.append(training_data[i-window:i, 0])\n",
    "        y_train.append(training_data[i, 0])\n",
    "    #^ this is a loop that trains the model based on window size\n",
    "\n",
    "    x_train, y_train = np.array(x_train), np.array(y_train) #turn the data back into maliable arrays\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1)) #Account for discrepencies in data\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True) #Stop training if the model stops improving\n",
    "\n",
    "    model = keras.models.Sequential() #create the model itself and set it as a sequential framework.\n",
    "        #^ Imported from TensorFlow's Keras library--open source\n",
    "    model.add(keras.layers.LSTM(64, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
    "        #^ Build a hidden layer, this is what sets it as a RNN\n",
    "    model.add(keras.layers.LSTM(64, return_sequences=False))\n",
    "        #^ Same thing, adding detail\n",
    "    model.add(keras.layers.Dense(128, activation='relu'))\n",
    "        #^ Memory\n",
    "    model.add(keras.layers.Dropout(.5))\n",
    "        #^ Forget Layer\n",
    "    model.add(keras.layers.Dense(1))\n",
    "        #^ Another Dense Layer\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='mae',\n",
    "                  metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "        #^ Put them all together and analyze loss\n",
    "\n",
    "    model.fit(x_train, y_train, batch_size=32, epochs=20, verbose=1, callbacks=[early_stop]) #Run the training. Adjust based on needs\n",
    "\n",
    "    test_data = scaled_data[training_data_len - window:]\n",
    "    x_test = []\n",
    "    y_test = dataset[training_data_len:]\n",
    "\n",
    "    for i in range(window, len(test_data)):\n",
    "        x_test.append(test_data[i-window:i, 0])\n",
    "\n",
    "    x_test = np.array(x_test)\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "    predictions = model.predict(x_test)\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "    # Create a DataFrame with dates and predictions\n",
    "    prediction_dates = df['Date'][training_data_len:].reset_index(drop=True)\n",
    "    predictions_df = pd.DataFrame({'Date': prediction_dates, 'Predictions': predictions.flatten()})\n",
    "\n",
    "    # Calculate Mean Absolute Percentage Error (MAPE)\n",
    "    y_true = df[column_name][training_data_len:].reset_index(drop=True)\n",
    "    y_pred = predictions_df['Predictions']\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "\n",
    "    return predictions_df, mape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d01f19",
   "metadata": {},
   "source": [
    "Run the one step ahead algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df, mape = predict_and_evaluate_column('DGS10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65a895f",
   "metadata": {},
   "source": [
    "Model for true backtest/forecast. \n",
    "\n",
    "Differences:\n",
    "- This prediction model appends the algorithm based on the prediction values, essentially using the same sliding window system as the one step ahead model, but uses predictive models.\n",
    "- This model predicts the nect value, appends it to the input window, and repeatst this for the length of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86a40c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import RandomSearch\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model(hp, input_shape):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.LSTM(\n",
    "        units=hp.Int('lstm_units', min_value=32, max_value=128, step=32),\n",
    "        return_sequences=True,\n",
    "        input_shape=input_shape\n",
    "    ))\n",
    "    model.add(keras.layers.LSTM(\n",
    "        units=hp.Int('lstm_units2', min_value=32, max_value=128, step=32),\n",
    "        return_sequences=False\n",
    "    ))\n",
    "    model.add(keras.layers.Dense(\n",
    "        units=hp.Int('dense_units', min_value=64, max_value=256, step=64),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(keras.layers.Dropout(\n",
    "        rate=hp.Float('dropout', min_value=0.2, max_value=0.7, step=0.1)\n",
    "    ))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float('lr', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "        loss='mae',\n",
    "        metrics=[keras.metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def predict_and_evaluate_column_true_backtest_tuned(column_name):\n",
    "    window = 60\n",
    "    training_data_percent = .8\n",
    "\n",
    "    industry = df.filter([column_name])\n",
    "    dataset = industry.values\n",
    "    training_data_len = int(np.ceil(len(dataset) * training_data_percent))\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(dataset)\n",
    "\n",
    "    training_data = scaled_data[:training_data_len]\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "    for i in range(window, len(training_data)):\n",
    "        x_train.append(training_data[i-window:i, 0])\n",
    "        y_train.append(training_data[i, 0])\n",
    "\n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "    tuner = RandomSearch(\n",
    "        lambda hp: build_model(hp, (x_train.shape[1], 1)),\n",
    "        objective='loss',\n",
    "        max_trials=10,\n",
    "        executions_per_trial=1,\n",
    "        directory='tuner_dir',\n",
    "        project_name='fred_lstm'\n",
    "    )\n",
    "\n",
    "    tuner.search(x_train, y_train, epochs=20, batch_size=32, verbose=1)\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "    test_len = len(scaled_data) - training_data_len\n",
    "    input_seq = scaled_data[training_data_len-window:training_data_len, 0].copy()\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(test_len):\n",
    "        x_input = np.array(input_seq).reshape((1, window, 1))\n",
    "        pred = best_model.predict(x_input)\n",
    "        predictions.append(pred[0, 0])\n",
    "        input_seq = np.append(input_seq[1:], pred[0, 0])\n",
    "\n",
    "    predictions = np.array(predictions).reshape(-1, 1)\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "    prediction_dates = df['Date'][training_data_len:].reset_index(drop=True)\n",
    "    predictions_df = pd.DataFrame({'Date': prediction_dates, 'Predictions': predictions.flatten()})\n",
    "\n",
    "    y_true = df[column_name][training_data_len:].reset_index(drop=True)\n",
    "    y_pred = predictions_df['Predictions']\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "    return predictions_df, mape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bf3717",
   "metadata": {},
   "source": [
    "Run the true backtest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfad5ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df, mape = predict_and_evaluate_column_true_backtest_tuned('DGS10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a267f01",
   "metadata": {},
   "source": [
    "Function that plots all training data against the predictions and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a449347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(column_name, predictions_df, mape):\n",
    "    \"\"\"\n",
    "    Plots the actual data and predictions for a given column and prints the MAPE.\n",
    "\n",
    "    Args:\n",
    "        column_name (str): The name of the column being plotted.\n",
    "        predictions_df (pandas.DataFrame): DataFrame with 'Date' and 'Predictions' columns.\n",
    "        mape (float): The Mean Absolute Percentage Error (MAPE) for the column.\n",
    "    \"\"\"\n",
    "    training_data_percent = .8\n",
    "\n",
    "    train = df[:int(np.ceil( len(df) * training_data_percent ))]\n",
    "    test = df[int(np.ceil( len(df) * training_data_percent )):]\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(train['Date'], train[column_name], label='Training Data', color='blue')\n",
    "    plt.plot(test['Date'], test[column_name], label='Test Data', color='orange')\n",
    "    plt.plot(predictions_df['Date'], predictions_df['Predictions'], label='Predictions', color='green')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(column_name)\n",
    "    plt.title(f'{column_name} Data')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(f\"Mean Absolute Percentage Error (MAPE) for {column_name}: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c7d750",
   "metadata": {},
   "source": [
    "Call plot_predictions function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7349a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions('DGS10', predictions_df, mape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
